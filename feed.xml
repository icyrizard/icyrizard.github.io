<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-03-01T23:30:38+01:00</updated><id>/feed.xml</id><title type="html">CodeDerps</title><subtitle>A collection guides and thoughts about programming, about things I've personally encountered through the years, years of study and working experience.
</subtitle><author><name>Richard Torenvliet</name><email>richard@torenvliet.org</email></author><entry><title type="html">How to split up Docker files (For Faster Deploys)</title><link href="/aws/docker/split/up/file/2021/03/01/splitting-up-docker-files.html" rel="alternate" type="text/html" title="How to split up Docker files (For Faster Deploys)" /><published>2021-03-01T07:01:36+01:00</published><updated>2021-03-01T07:01:36+01:00</updated><id>/aws/docker/split/up/file/2021/03/01/splitting-up-docker-files</id><content type="html" xml:base="/aws/docker/split/up/file/2021/03/01/splitting-up-docker-files.html">&lt;p&gt;It’s now a been a view years back that I’ve come across Docker, and I’ve almost in all my projects ever since. For a 
long while I was working on small projects, be it either hobby projects, study projects and small paid ones. During all this time
I actually never got bothered by the amount of statements in my Dockerfile. That was until I started integrating Docker into
the project of my current job. This project has been actively developed on for 8-9 years before I started moving it to Docker. It wasn’t 
a small project to say the least.&lt;/p&gt;

&lt;p&gt;I started naively with one Dockerfile, it soon turned out to contain around 200 lines. This however, I didn’t see as a problem. Let’s
cover the problem in the next Section.&lt;/p&gt;

&lt;h3 id=&quot;the-problem-with-huge-dockerfiles&quot;&gt;The Problem with Huge Dockerfiles&lt;/h3&gt;
&lt;p&gt;For those that are not aware of how Dockerfiles work, each statement in a Dockerfile creates a new layer. This new layer
can be seen as one step of the build stage, each layer will add either complexity and will add more time to the build time.
The actual time a new layer adds to the total build time Dockerfile of course depends on the statement itself, but they all
take time, and it’s hard to predict in advance which one of them takes longer than the other.&lt;/p&gt;

&lt;p&gt;So, if each layer will add complexity to the grant total of the build, given enough layers, the build time will grow unmanageably long.
Also, the unnecessary work that needs to be done each time a build is run feels painful. The build gets slow, we wasted the energy, and it’s not even environment friendly come
to think of it…&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note that I’m not going to cover the build size in this post, which is an important aspect of fast deployments, but the method
discussed in this post will take care of this too (indirectly).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The funny thing is that you won’t notice this slowdown when developing locally. When building a Docker Image on your machine, 
Docker will remember the incremental layers on the disk such that it won’t run a layer
a second time [2]. However, if you take use a build service like &lt;a href=&quot;https://travis-ci.com/&quot;&gt;Travis[2]&lt;/a&gt;, these layers do not remain anywhere after a build is done. I have tried
using Travis it’s caching mechanism to cache certain parts of the build, but this was completely not working for me. Somehow
the cache on Travis was busted each time after running a build. Either way, this solution was just adding too much complexity, even if it were to work.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;An interesting read on how this is implemented in Docker can be found here in their Documentation: &lt;a href=&quot;https://docs.docker.com/storage/storagedriver/overlayfs-driver/#how-the-overlay2-driver-works&quot;&gt;How the Overlay2 Driver Works[3]&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;show-me-an-example&quot;&gt;Show me an Example!&lt;/h3&gt;
&lt;p&gt;Without further discussing the actual problem, lets workout an example that we can use to split up Builds. Consider the following
Dockerfile.&lt;/p&gt;

&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; python:3.7.5&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apt-get update &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; a b c ... x y z

&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; requirements.txt /tmp&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; /tmp/requirements.txt
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-U&lt;/span&gt; pytest

&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /src&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;lets-split-it-up-using-multi-stage-builds&quot;&gt;Let’s split it up! Using Multi-Stage Builds&lt;/h3&gt;
&lt;p&gt;To split up the Dockerfile, we use Dockers’ &lt;a href=&quot;https://docs.docker.com/develop/develop-images/multistage-build/&quot;&gt;Multi-Stage builds&lt;/a&gt;
It allows you to create multiple stages that can be used and copied from separately, using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM &amp;lt;some_image:verwsion&amp;gt; as &amp;lt;Some Name&amp;gt;&lt;/code&gt; statements.&lt;/p&gt;

&lt;p&gt;Everything after such a statement until the next one is called stage and can be targeted. Remember the notion of targeting, as it gets more 
interesting after this example.&lt;/p&gt;

&lt;h3 id=&quot;we-need-some-rules-to-make-it-easier&quot;&gt;We need some Rules to make it easier&lt;/h3&gt;
&lt;p&gt;I’ve used two simple rules to make splitting a bit easier. They are not perfect, but they give a direction for optimization.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Rule 1:&lt;/strong&gt; Define your Rutime dependencies. Like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt-gets&lt;/code&gt; are runtime dependencies, they are needed on a machine level.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Rule 2:&lt;/strong&gt; Define your 3rdparty dependencies. Statements like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install -r requirements.txt&lt;/code&gt; are 3Party dependencies that normally more on an application level.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# [1] Rename the Dockerfile this to our 'base' image.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; python:3.7.5 as base &lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apt-get update &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; something

&lt;span class=&quot;c&quot;&gt;# [2]: Create a new image layer here, and use 'base' as the base image.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; base as thirdPartyDeps&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; requirements.txt /tmp&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; /tmp/requirements.txt

&lt;span class=&quot;c&quot;&gt;# [3]: Create an image that is used for testing only.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; thirdPartyDeps as testRunner&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;pip &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-U&lt;/span&gt; pytest

&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; thirdPartyDeps as src&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# [4] add the src&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ADD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; src/ /var/www/html/&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; src as acceptance&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## add more acc deps&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; src as production&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Remove some unessary runtime deps.&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;RUN &lt;/span&gt;apt-get purge &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; a b c 
&lt;span class=&quot;c&quot;&gt;## add more or remove maybe.&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;WORKDIR&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; /src&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note that some runtime dependencies are sometimes only needed because &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install&lt;/code&gt; requires them during installation.
You can easily remove them in the designated build to reduce the image size.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That’s useful, but we still face some problems. Running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker build&lt;/code&gt; without a cache, will go over all the lines in the Dockerfile. 
So, lets say we want to start building a production image. For this we will use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--target&lt;/code&gt; command as shown in the Docker Documentation &lt;a href=&quot;#references&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build --tag production:latest --target production --file Dockerfile .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Listings 3. Showing how to build a production target build. All of the layers will get executed if no cache is available.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The missing link is to tag the other stages with a version and upload them to a private registry. Having a private 
registry containing specific tagged images will allow you to ‘fake’ a cache. Meaning that the layers will not be executed
but will be downloaded. Luckily the network speed is not a bottleneck on our CI-tool :)&lt;/p&gt;

&lt;h3 id=&quot;splitting-up-the-dockerfile-itself&quot;&gt;Splitting up the Dockerfile itself&lt;/h3&gt;
&lt;p&gt;Splitting up the Dockerfile into different files allows for better oversight on the problem. Each Dockerfile will be
a separate tagged image that can be used by a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM ..&lt;/code&gt; statement. Here, a simplified example of splitting that file up in to 3 Dockerfiles is shown by the following:
Later I’ll show how this used further.&lt;/p&gt;

&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Dockerfile.base&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; python:3.7.5 as base &lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# =======================&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Dockerfile.runtime-deps&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; base as thirdPartyDeps&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# =======================&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Dockerfile&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; thirdPartyDeps as testRunner&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; thirdPartyDeps as acc&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; thirdPartyDeps as prd&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# =======================&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;Multi-stage builds&lt;/li&gt;
  &lt;li&gt;Split and tag and build Multiple Dockerfiles&lt;/li&gt;
  &lt;li&gt;Build Arguments with our Registry Target.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;not-there-yet-we-need-it-in-production&quot;&gt;Not there yet, we need it in Production!&lt;/h3&gt;
&lt;p&gt;Remember that the &lt;a href=&quot;#the-problem-with-huge-dockerfiles&quot;&gt;problem&lt;/a&gt;, we need to make this work on our CI-tool where we can’t use a cache.
By making separate images, tagging them nicely with a version AND upload them to a private registry, only then are starting
to speed up the process. For this, we can use another that feature that Docker provides: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Build Arguments&lt;/code&gt;. They are arguments
that you can pass to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker build&lt;/code&gt; which replaces variables in our Dockerfile.&lt;/p&gt;

&lt;div class=&quot;language-Dockerfile highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; THIRD_PARTY_DEPS_REGISTRY&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ARG&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; RUNTIME_DEPS_REGISTRY&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Rename the images for convenience&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; $THIRD_PARTY_DEPS_REGISTRY as thirdPartyDeps&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; $RUNTIME_DEPS_REGISTRY as base&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; base as baseWithSrcAndThirdPartyDeps&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Copy 3rd party installs to the src&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;COPY&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; --from=thirdParty /3rdparty /3rdparty&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ADD&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; src/ /var/www/html&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Dockerfile&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; baseWithSrcAndThirdPartyDeps as testRunner&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; baseWithSrcAndThirdPartyDeps as acc&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; baseWithSrcAndThirdPartyDeps as prd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note that I left out the Dockerfile examples for the runtime dependencies, and the 3rdparty package installs to keep
the information more compact.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;build-the-images&quot;&gt;Build the images&lt;/h3&gt;
&lt;p&gt;Set the registry URL first.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;REGISTRY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;my-registry-domain.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Create the base image&lt;/strong&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -t base -t {REGISTERY}:base:v1.0.0 --file Docker.base .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Create the runtime dependencies&lt;/strong&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -t runtime-deps -t {REGISTERY}:runtime_deps:v1.0.0 --file Docker.runtime-deps .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Create an acceptance build&lt;/strong&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -t acceptance:$(date +&quot;%Y%m%d%M%S&quot;) \
 --build-arg BASE={REGISTERY}:base:v1.0.0 \
 --build-arg RUNTIME_DEPS_REGISTRY={REGISTERY}:runtime-deps:v1.0.0 \
 --target production --file Dockerfile .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Create a production build&lt;/strong&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker build -t production:$(date +&quot;%Y%m%d%M%S&quot;) \
 --build-arg BASE={REGISTERY}:base:v1.0.0 \
 --build-arg RUNTIME_DEPS_REGISTRY={REGISTERY}:runtime-deps:v1.0.0 \
 --target production --file Dockerfile .
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;h4 id=&quot;1-multi-stage-builds&quot;&gt;1. &lt;a href=&quot;https://docs.docker.com/develop/develop-images/multistage-build/&quot;&gt;Multi-Stage builds&lt;/a&gt;&lt;/h4&gt;
&lt;h4 id=&quot;2-travis&quot;&gt;2. &lt;a href=&quot;https://travis-ci.com/&quot;&gt;Travis&lt;/a&gt;&lt;/h4&gt;
&lt;h4 id=&quot;3-how-the-overlay2-driver-works&quot;&gt;3. &lt;a href=&quot;https://docs.docker.com/storage/storagedriver/overlayfs-driver/#how-the-overlay2-driver-works&quot;&gt;How the Overlay2 Driver Works&lt;/a&gt;&lt;/h4&gt;</content><author><name>Richard Torenvliet</name><email>richard@torenvliet.org</email></author><category term="aws" /><category term="docker" /><category term="split" /><category term="up" /><category term="file" /><summary type="html">It’s now a been a view years back that I’ve come across Docker, and I’ve almost in all my projects ever since. For a long while I was working on small projects, be it either hobby projects, study projects and small paid ones. During all this time I actually never got bothered by the amount of statements in my Dockerfile. That was until I started integrating Docker into the project of my current job. This project has been actively developed on for 8-9 years before I started moving it to Docker. It wasn’t a small project to say the least. I started naively with one Dockerfile, it soon turned out to contain around 200 lines. This however, I didn’t see as a problem. Let’s cover the problem in the next Section. The Problem with Huge Dockerfiles For those that are not aware of how Dockerfiles work, each statement in a Dockerfile creates a new layer. This new layer can be seen as one step of the build stage, each layer will add either complexity and will add more time to the build time. The actual time a new layer adds to the total build time Dockerfile of course depends on the statement itself, but they all take time, and it’s hard to predict in advance which one of them takes longer than the other. So, if each layer will add complexity to the grant total of the build, given enough layers, the build time will grow unmanageably long. Also, the unnecessary work that needs to be done each time a build is run feels painful. The build gets slow, we wasted the energy, and it’s not even environment friendly come to think of it… Note that I’m not going to cover the build size in this post, which is an important aspect of fast deployments, but the method discussed in this post will take care of this too (indirectly). The funny thing is that you won’t notice this slowdown when developing locally. When building a Docker Image on your machine, Docker will remember the incremental layers on the disk such that it won’t run a layer a second time [2]. However, if you take use a build service like Travis[2], these layers do not remain anywhere after a build is done. I have tried using Travis it’s caching mechanism to cache certain parts of the build, but this was completely not working for me. Somehow the cache on Travis was busted each time after running a build. Either way, this solution was just adding too much complexity, even if it were to work. An interesting read on how this is implemented in Docker can be found here in their Documentation: How the Overlay2 Driver Works[3] Show me an Example! Without further discussing the actual problem, lets workout an example that we can use to split up Builds. Consider the following Dockerfile. FROM python:3.7.5 RUN apt-get update -y &amp;amp;&amp;amp; apt-get install -y a b c ... x y z COPY requirements.txt /tmp RUN pip install -r /tmp/requirements.txt RUN pip install -U pytest WORKDIR /src Let’s split it up! Using Multi-Stage Builds To split up the Dockerfile, we use Dockers’ Multi-Stage builds It allows you to create multiple stages that can be used and copied from separately, using FROM &amp;lt;some_image:verwsion&amp;gt; as &amp;lt;Some Name&amp;gt; statements. Everything after such a statement until the next one is called stage and can be targeted. Remember the notion of targeting, as it gets more interesting after this example. We need some Rules to make it easier I’ve used two simple rules to make splitting a bit easier. They are not perfect, but they give a direction for optimization. Rule 1: Define your Rutime dependencies. Like apt-gets are runtime dependencies, they are needed on a machine level. Rule 2: Define your 3rdparty dependencies. Statements like pip install -r requirements.txt are 3Party dependencies that normally more on an application level. # [1] Rename the Dockerfile this to our 'base' image. FROM python:3.7.5 as base RUN apt-get update -y &amp;amp;&amp;amp; apt-get install -y something # [2]: Create a new image layer here, and use 'base' as the base image. FROM base as thirdPartyDeps COPY requirements.txt /tmp RUN pip install -r /tmp/requirements.txt # [3]: Create an image that is used for testing only. FROM thirdPartyDeps as testRunner RUN pip install -U pytest FROM thirdPartyDeps as src # [4] add the src ADD src/ /var/www/html/ FROM src as acceptance ## add more acc deps FROM src as production # Remove some unessary runtime deps. RUN apt-get purge -y a b c ## add more or remove maybe. WORKDIR /src Note that some runtime dependencies are sometimes only needed because pip install requires them during installation. You can easily remove them in the designated build to reduce the image size. That’s useful, but we still face some problems. Running docker build without a cache, will go over all the lines in the Dockerfile. So, lets say we want to start building a production image. For this we will use the --target command as shown in the Docker Documentation [1]. docker build --tag production:latest --target production --file Dockerfile . Listings 3. Showing how to build a production target build. All of the layers will get executed if no cache is available. The missing link is to tag the other stages with a version and upload them to a private registry. Having a private registry containing specific tagged images will allow you to ‘fake’ a cache. Meaning that the layers will not be executed but will be downloaded. Luckily the network speed is not a bottleneck on our CI-tool :) Splitting up the Dockerfile itself Splitting up the Dockerfile into different files allows for better oversight on the problem. Each Dockerfile will be a separate tagged image that can be used by a FROM .. statement. Here, a simplified example of splitting that file up in to 3 Dockerfiles is shown by the following: Later I’ll show how this used further. # Dockerfile.base FROM python:3.7.5 as base # ======================= # Dockerfile.runtime-deps FROM base as thirdPartyDeps # ======================= # Dockerfile FROM thirdPartyDeps as testRunner FROM thirdPartyDeps as acc FROM thirdPartyDeps as prd # ======================= Multi-stage builds Split and tag and build Multiple Dockerfiles Build Arguments with our Registry Target. Not there yet, we need it in Production! Remember that the problem, we need to make this work on our CI-tool where we can’t use a cache. By making separate images, tagging them nicely with a version AND upload them to a private registry, only then are starting to speed up the process. For this, we can use another that feature that Docker provides: Build Arguments. They are arguments that you can pass to docker build which replaces variables in our Dockerfile. ARG THIRD_PARTY_DEPS_REGISTRY ARG RUNTIME_DEPS_REGISTRY ## Rename the images for convenience FROM $THIRD_PARTY_DEPS_REGISTRY as thirdPartyDeps FROM $RUNTIME_DEPS_REGISTRY as base FROM base as baseWithSrcAndThirdPartyDeps ## Copy 3rd party installs to the src COPY --from=thirdParty /3rdparty /3rdparty ADD src/ /var/www/html # Dockerfile FROM baseWithSrcAndThirdPartyDeps as testRunner FROM baseWithSrcAndThirdPartyDeps as acc FROM baseWithSrcAndThirdPartyDeps as prd Note that I left out the Dockerfile examples for the runtime dependencies, and the 3rdparty package installs to keep the information more compact. Build the images Set the registry URL first. REGISTRY=my-registry-domain.com Create the base image docker build -t base -t {REGISTERY}:base:v1.0.0 --file Docker.base . Create the runtime dependencies docker build -t runtime-deps -t {REGISTERY}:runtime_deps:v1.0.0 --file Docker.runtime-deps . Create an acceptance build docker build -t acceptance:$(date +&quot;%Y%m%d%M%S&quot;) \ --build-arg BASE={REGISTERY}:base:v1.0.0 \ --build-arg RUNTIME_DEPS_REGISTRY={REGISTERY}:runtime-deps:v1.0.0 \ --target production --file Dockerfile . Create a production build docker build -t production:$(date +&quot;%Y%m%d%M%S&quot;) \ --build-arg BASE={REGISTERY}:base:v1.0.0 \ --build-arg RUNTIME_DEPS_REGISTRY={REGISTERY}:runtime-deps:v1.0.0 \ --target production --file Dockerfile . References 1. Multi-Stage builds 2. Travis 3. How the Overlay2 Driver Works</summary></entry><entry><title type="html">Notes on setting up AWS S3 Pre-signed URLs</title><link href="/aws/presigned-urls/s3/devops/2021/01/29/presigned-urls.html" rel="alternate" type="text/html" title="Notes on setting up AWS S3 Pre-signed URLs" /><published>2021-01-29T19:48:16+01:00</published><updated>2021-01-29T19:48:16+01:00</updated><id>/aws/presigned-urls/s3/devops/2021/01/29/presigned-urls</id><content type="html" xml:base="/aws/presigned-urls/s3/devops/2021/01/29/presigned-urls.html">&lt;p&gt;A while I’ve been working on a project to further upgrade the security for the company that I work for. I still 
remember some struggles I’ve endured during the project, both by design and implementation.&lt;/p&gt;

&lt;p&gt;This blog post will cover some of my learnings during this project. The examples will be in PHP, while I do have my 
opinion about PHP, but it’s the safest way for me to guarantee that my examples work :).&lt;/p&gt;

&lt;h2 id=&quot;a-little-background-story&quot;&gt;A Little Background Story&lt;/h2&gt;
&lt;p&gt;Users directly upload pictures to our Backend Application, during the upload process they are resized into two types of sizes. A thumbnail
size and a preview size. This yields 3 images that will need storing in S3, each will be get a long string of random characters as its new
name in S3 bucket. This should alreayd be fairly secure, there is no way on earth someone could guess the names of these images. Even given the fact that
we store an insane amount of images, our human brain cannot comprehend the size of the search space in which an attacker
has to go through to be very very lucky…&lt;/p&gt;

&lt;p&gt;That said, a fair remark I got was:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;“But uhh, what if the paths to the s3 bucket objects were leaked? Won’t they be downloadable forever?”.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is a truly valid remark, evil forces with the right access pass, will undoubtedly be able to pull something off like this.
So, in short, we need to block public access to our bucket. This can be done by either using Signed or Presigned URLS.&lt;/p&gt;

&lt;h2 id=&quot;what-are-signed-urls&quot;&gt;What are Signed URLS?&lt;/h2&gt;
&lt;p&gt;For the Signed URLs mechanism, the backend application will generate URLS that are signed, using a private key for which Cloudfront
has the public key. The link generated will
point towards a CloudFront Distribution that is able to verify the signing hidden in the requests parameters. The benefit
is that you can use Cloudfront and take use of its caching mechanisms as well as protect the resources it has to. It does require root access to the AWS Account,
so depending on your situation, this may be more difficult to implement.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;More will come&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;what-are-pre-signed-urls&quot;&gt;What are Pre-signed URLS?&lt;/h2&gt;
&lt;p&gt;AWS Pre-signed URLs allow for temporary granting access rights to someone with the link. This being, to rights to either &lt;strong&gt;read&lt;/strong&gt;, &lt;strong&gt;write&lt;/strong&gt; or &lt;strong&gt;delete&lt;/strong&gt;
S3 Objects. From a server side perspective, this could simply mean that we generate the Presigned URL on the server and that link to 
the client. The client can use the URLs to perform the desired operation, depending on the access right granted by the server.&lt;/p&gt;

&lt;p&gt;A prerequisites is that the AWS Service needs to have access to the location to the 
Bucket and Path for which it is generating a Pre-signed URL. You can either use an AWS Key and AWS Secret, 
but it’s far more secure and scalable solution to use an Iam Role that is attached to the AWS service of your backend entity in AWS.
To use it, you should use pass &lt;a href=&quot;https://docs.aws.amazon.com/sdk-for-php/v3/developer-guide/guide_configuration.html&quot;&gt;Aws\CacheInterface&lt;/a&gt;, 
as the credentials so setup the&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;S3Client&lt;/code&gt;. An example of this is shown in Listings 1.&lt;/p&gt;

&lt;p&gt;This will prevent long reuse of an AWS Key and Secret, this reduces the risk of those keys being leaked in any way and be active forever without knowing. 
Role based access will give you full power in what is allowed by that specific role. Another benefit is that is a less expensive operation.&lt;/p&gt;

&lt;h2 id=&quot;the-goal&quot;&gt;The Goal&lt;/h2&gt;
&lt;p&gt;In the end, we want users to use a link to our backend server and get automatically redirected to S3 with a presigned url or signed url.
The choice for the current implementation has fallen on the Pre-signed urls. It requires one less entity in the scheme of things,
and we do not need the access to the AWS root account to create a key pair, so for our situation Presigned URLS were our best pick.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The user logs in to our platform&lt;/li&gt;
  &lt;li&gt;Retrieves resources and their attachments&lt;/li&gt;
  &lt;li&gt;The attachments have links directly leading to our Backend.&lt;/li&gt;
  &lt;li&gt;We check if they have a valid login and if they are allowed to see the current requested resource using their access key.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The benefits of this method is that the links that are responded by our API can remain the same forever. Only upon downloading / accessing 
that url with the right access credentials, the generation of the Pre-signed URL starts. The HTTP Client automatically is redirected to the right URL.&lt;/p&gt;

&lt;p&gt;For an attacker it became a lot harder to find the actual links to images because that would mean to intercept network traffic
and capture the HTTP &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Location Header&lt;/code&gt; response of our server to the client. That is what how a the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/HTTP/Redirections&quot;&gt;HTTP Redirect&lt;/a&gt; 
mechanism actually works.&lt;/p&gt;

&lt;h3 id=&quot;aws-s3-client-initialization&quot;&gt;AWS S3 Client Initialization&lt;/h3&gt;
&lt;p&gt;This documentation is a bit misleading on what you should supply as the credentials option for the S3Client:&lt;/p&gt;

&lt;h5 id=&quot;copied-from-the-aws-docs-on-aws-s3-client-credentials-initialisation&quot;&gt;&lt;strong&gt;Copied from the AWS Docs on AWS S3 Client Credentials initialisation:&lt;/strong&gt;&lt;/h5&gt;
&lt;blockquote&gt;
  &lt;p&gt;If you don’t provide a credentials option, the SDK attempts to load credentials from your environment in the following order:&lt;/p&gt;
  &lt;ol&gt;
    &lt;li&gt;Load credentials from environment variables.&lt;/li&gt;
    &lt;li&gt;Load credentials from a credentials .ini file.&lt;/li&gt;
    &lt;li&gt;Load credentials from an IAM role.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;But if yoo provide an object of type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Aws\CacheInterface&lt;/code&gt;, you will get the AWS Key and Secret that the S3Client that is has gotten itself from the IAM Role. 
The AWS Key and Secret it has retrieved from the 
&lt;a href=&quot;https://docs.amazonaws.cn/zh_cn/aws-sdk-php/guide/latest/guide/credentials.html#using-iam-roles-for-amazon-ec2-container-service-tasks&quot;&gt;EC2 Service Metadata&lt;/a&gt;
, which is retrieved by means of a request. This is obviously slower than if were to store those keys temporarily which can be reused.&lt;/p&gt;

&lt;h3 id=&quot;getting-an-s3-object-with-a-presigned-url&quot;&gt;Getting an S3 Object with a Presigned URL&lt;/h3&gt;
&lt;p&gt;Following the Example on the &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/dev/RetrieveObjSingleOpPHP.html&quot;&gt;AWS Documentation on PresignedURLs&lt;/a&gt;,
and combining this the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Aws\CacheInterface&lt;/code&gt;, we get the following
result for retrieving an object using the presigned URL. Note that this not the result we are aiming for yet… we don’t want to
download the object in the Backend, we want to give the sweet task of the downloading the object to the client.&lt;/p&gt;

&lt;div class=&quot;language-php highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;?php&lt;/span&gt;

&lt;span class=&quot;cd&quot;&gt;/**
 * Use get and set to store your credentials in a Cache that can be quickly accessed.
 * 
* Class CacheObject
 */&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CacheObject&lt;/span&gt; &lt;span class=&quot;kd&quot;&gt;implements&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Aws\CacheInterface&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ttl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;mf&quot;&gt;...&lt;/span&gt; 

&lt;span class=&quot;nv&quot;&gt;$keyName&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'uploads/aaaaaaaaaaaaaaaaaaaaa.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$credentials&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CacheObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$s3Client&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Aws\S3\S3Client&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'version'&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'latest'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'region'&lt;/span&gt;      &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'eu-west-1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;s1&quot;&gt;'credentials'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$credentials&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$s3Client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getCommand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'GetObject'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s1&quot;&gt;'Bucket'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'derp-bucket'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s1&quot;&gt;'Key'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$keyName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$s3Client&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;createPresignedRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'+2 minutes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Listings 1: Shows an Example of how to get an S3 Object from S3 using a Pre-singed Request.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is nice, but we don’t want the actual file, we just need the URL and send an HTTP Redirect such the client can follow that link instead.&lt;/p&gt;

&lt;h3 id=&quot;getting-a-presigned-url&quot;&gt;Getting a Presigned URL.&lt;/h3&gt;
&lt;p&gt;Now remember that the Goal is to redirect the user to the S3 Bucket, after we have checked their login. So we have
to generate the Presigned URL by the following snippet:&lt;/p&gt;

&lt;div class=&quot;language-php highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$lifetime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$cmd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$s3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getCommand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'GetObject'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s1&quot;&gt;'Bucket'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'derp-bucket'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s1&quot;&gt;'Key'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$keyName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$presignedUrlRequest&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;S3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;createPresignedRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$cmd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DateUtils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$lifetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$presignedUrl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$presignedUrlRequest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getUri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Listings 2: Shows an Example of how to get the Pre-signed Url from S3.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An important part was to cast the result of the getUri function to a String. It returns an object that contains
a toString method. Not converting it will give you hours of fun debugging, you’re welcome :).&lt;/p&gt;
&lt;div class=&quot;language-php highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$presignedUrl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$presignedUrlRequest&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getUri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;sending-the-redirect-to-the-client&quot;&gt;Sending the Redirect to the Client&lt;/h3&gt;
&lt;p&gt;Now that we have the URL we can send a 303 to with the location header set to the Presigned URL. The HTTP Client of
the user will automatically follow the redirect and start downloading the S3 Object. Now at this moment a problem arose.
This problem is discussed in the following Section.&lt;/p&gt;

&lt;h2 id=&quot;how-long-should-the-pre-signed-url-lifetime-be&quot;&gt;How long should the Pre-signed URL Lifetime be?&lt;/h2&gt;
&lt;p&gt;Talking from own experience (I could definitely be wrong about this), the lifetime of the Pre-signed URL must be around 60 seconds at least.
Any lower will occasionally trigger &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Expired&lt;/code&gt; errors by S3. This could either be due differences in clocks, or the time difference between getting a fresh new Pre-signed URL and when
the download actually starts can be a cause of an Expired response from S3.&lt;/p&gt;

&lt;p&gt;Remind that we’re trying to block the fact that S3 Objects are downloadable forever if you have the link,
a hacker that has obtained the Pre-signed URL somehow can download the Object within the same second of finding it.&lt;/p&gt;

&lt;h4 id=&quot;the-authorization-header-problem&quot;&gt;The Authorization Header Problem&lt;/h4&gt;
&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Authorization Header&lt;/code&gt; is not usable now. You cannot use the Authorization Header to authenticate the user on your platform
AND send a redirect to AWS. Strangely enough, the Authorization Header is not stripped after a redirect, see &lt;a href=&quot;#references&quot;&gt;[1]&lt;/a&gt;. That’s something I never realized. 
And who would until they find out the hard way right?&lt;/p&gt;

&lt;p&gt;Anyway, to prevent this, we go back to our old school Cookie Authorization. So instead of the Authorization Header that contains the acces key, they must use send 
a Cookie to Authorize themselves on our server to get access to S3 Object. A well known fact about Cookies is that they are automatically stripped if the domain changes.&lt;/p&gt;

&lt;p&gt;The following Error is given when trying to send the Authorization Header (unknowingly…).&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;Error&amp;gt;&amp;lt;Code&amp;gt;&lt;/span&gt;InvalidArgument&lt;span class=&quot;nt&quot;&gt;&amp;lt;/Code&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;Message&amp;gt;&lt;/span&gt;Only one auth mechanism allowed; only the X-Amz-Algorithm query parameter, Signature query string parameter or the Authorization header should be specified&lt;span class=&quot;nt&quot;&gt;&amp;lt;/Message&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;ArgumentName&amp;gt;&lt;/span&gt;Authorization&lt;span class=&quot;nt&quot;&gt;&amp;lt;/ArgumentName&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;ArgumentValue&amp;gt;&lt;/span&gt;Bearer Token&lt;span class=&quot;nt&quot;&gt;&amp;lt;/ArgumentValue&amp;gt;&amp;lt;RequestId&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/RequestId&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;HostId&amp;gt;&lt;/span&gt;...&lt;span class=&quot;nt&quot;&gt;&amp;lt;/HostId&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/Error&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Listings 3: Shows the error message received upon the HTTP Redirect where the Authorization Header was not stripped from the request.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the Pre-signed URL the X-Amz-Algorithm contains&lt;/p&gt;

&lt;h4 id=&quot;fix-the-cors-problem&quot;&gt;Fix the CORS problem&lt;/h4&gt;
&lt;p&gt;Needless to say, this problem needs solving I will state this for the sake of completeness, but for this problem no particular
weirdness happened for me. Lucky me. A small note, for our purpose only the GET method is needed, and restricting the origin is 
impossible and unnecessary. Impossible because our users should be able to use their own scripts to download images.&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;CORSConfiguration&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;xmlns=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;CORSRule&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;AllowedOrigin&amp;gt;&lt;/span&gt;*&lt;span class=&quot;nt&quot;&gt;&amp;lt;/AllowedOrigin&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;AllowedMethod&amp;gt;&lt;/span&gt;GET&lt;span class=&quot;nt&quot;&gt;&amp;lt;/AllowedMethod&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;MaxAgeSeconds&amp;gt;&lt;/span&gt;3000&lt;span class=&quot;nt&quot;&gt;&amp;lt;/MaxAgeSeconds&amp;gt;&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;AllowedHeader&amp;gt;&lt;/span&gt;*&lt;span class=&quot;nt&quot;&gt;&amp;lt;/AllowedHeader&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/CORSRule&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/CORSConfiguration&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;Listings 4: THe CORS Headers needed for on our S3 Bucket. Enter this XML into the ‘Permissions’ tab -&amp;gt; CORS Section of the S3 Bucket.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When you use do restrict the S3 bucket to certain domains, it can aid to a higher level of security. When your requirement is to allow access from anywhere to the
S3 bucket, this becomes impossible. That said, the Signed URL version of this using Cloudfront does give you the opportunity to strictly allow
access from that Cloudfront origin.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In these notes I’ve aimed to give an example of how Pre-signed Urls work and how they can be used in a production environment.
I’ve copied and put together some AWS Documentation to make small example of how Pre-signed Urls could
work for an application. We’ve seen an example of how to set up the S3 Client efficiently using the Cache Object, an example
that I haven’t really seen on the AWS Documentation.&lt;/p&gt;

&lt;p&gt;We’ve seen the problem that occur when using an HTTP Redirect Request, that it does not strip out the Authorization 
Header after Redirection. In my opinion HTTP Client should really strip those out to prevent any dangerous leakage
of Access Keys. The Authorization Header is the modern version of the Cookie Header, and yet it is less secure considering this.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;h4 id=&quot;1-how-to-remove-authorization-header-in-a-http-302-response---stack-&quot;&gt;1. “How to remove authorization header in a http 302 response - Stack ….”&lt;/h4&gt;
&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/35400943/how-to-remove-authorization-header-in-a-http-302-response/45217599. Accessed 9 Feb. 2021.&quot;&gt;https://stackoverflow.com/questions/35400943/how-to-remove-authorization-header-in-a-http-302-response/45217599. Accessed 9 Feb. 2021.&lt;/a&gt;&lt;/p&gt;</content><author><name>Richard Torenvliet</name><email>richard@torenvliet.org</email></author><category term="aws" /><category term="presigned-urls" /><category term="s3" /><category term="devops" /><summary type="html">A while I’ve been working on a project to further upgrade the security for the company that I work for. I still remember some struggles I’ve endured during the project, both by design and implementation. This blog post will cover some of my learnings during this project. The examples will be in PHP, while I do have my opinion about PHP, but it’s the safest way for me to guarantee that my examples work :). A Little Background Story Users directly upload pictures to our Backend Application, during the upload process they are resized into two types of sizes. A thumbnail size and a preview size. This yields 3 images that will need storing in S3, each will be get a long string of random characters as its new name in S3 bucket. This should alreayd be fairly secure, there is no way on earth someone could guess the names of these images. Even given the fact that we store an insane amount of images, our human brain cannot comprehend the size of the search space in which an attacker has to go through to be very very lucky… That said, a fair remark I got was: “But uhh, what if the paths to the s3 bucket objects were leaked? Won’t they be downloadable forever?”. This is a truly valid remark, evil forces with the right access pass, will undoubtedly be able to pull something off like this. So, in short, we need to block public access to our bucket. This can be done by either using Signed or Presigned URLS. What are Signed URLS? For the Signed URLs mechanism, the backend application will generate URLS that are signed, using a private key for which Cloudfront has the public key. The link generated will point towards a CloudFront Distribution that is able to verify the signing hidden in the requests parameters. The benefit is that you can use Cloudfront and take use of its caching mechanisms as well as protect the resources it has to. It does require root access to the AWS Account, so depending on your situation, this may be more difficult to implement. More will come What are Pre-signed URLS? AWS Pre-signed URLs allow for temporary granting access rights to someone with the link. This being, to rights to either read, write or delete S3 Objects. From a server side perspective, this could simply mean that we generate the Presigned URL on the server and that link to the client. The client can use the URLs to perform the desired operation, depending on the access right granted by the server. A prerequisites is that the AWS Service needs to have access to the location to the Bucket and Path for which it is generating a Pre-signed URL. You can either use an AWS Key and AWS Secret, but it’s far more secure and scalable solution to use an Iam Role that is attached to the AWS service of your backend entity in AWS. To use it, you should use pass Aws\CacheInterface, as the credentials so setup theS3Client. An example of this is shown in Listings 1. This will prevent long reuse of an AWS Key and Secret, this reduces the risk of those keys being leaked in any way and be active forever without knowing. Role based access will give you full power in what is allowed by that specific role. Another benefit is that is a less expensive operation. The Goal In the end, we want users to use a link to our backend server and get automatically redirected to S3 with a presigned url or signed url. The choice for the current implementation has fallen on the Pre-signed urls. It requires one less entity in the scheme of things, and we do not need the access to the AWS root account to create a key pair, so for our situation Presigned URLS were our best pick. The user logs in to our platform Retrieves resources and their attachments The attachments have links directly leading to our Backend. We check if they have a valid login and if they are allowed to see the current requested resource using their access key. The benefits of this method is that the links that are responded by our API can remain the same forever. Only upon downloading / accessing that url with the right access credentials, the generation of the Pre-signed URL starts. The HTTP Client automatically is redirected to the right URL. For an attacker it became a lot harder to find the actual links to images because that would mean to intercept network traffic and capture the HTTP Location Header response of our server to the client. That is what how a the HTTP Redirect mechanism actually works. AWS S3 Client Initialization This documentation is a bit misleading on what you should supply as the credentials option for the S3Client: Copied from the AWS Docs on AWS S3 Client Credentials initialisation: If you don’t provide a credentials option, the SDK attempts to load credentials from your environment in the following order: Load credentials from environment variables. Load credentials from a credentials .ini file. Load credentials from an IAM role. But if yoo provide an object of type Aws\CacheInterface, you will get the AWS Key and Secret that the S3Client that is has gotten itself from the IAM Role. The AWS Key and Secret it has retrieved from the EC2 Service Metadata , which is retrieved by means of a request. This is obviously slower than if were to store those keys temporarily which can be reused. Getting an S3 Object with a Presigned URL Following the Example on the AWS Documentation on PresignedURLs, and combining this the Aws\CacheInterface, we get the following result for retrieving an object using the presigned URL. Note that this not the result we are aiming for yet… we don’t want to download the object in the Backend, we want to give the sweet task of the downloading the object to the client. &amp;lt;?php /** * Use get and set to store your credentials in a Cache that can be quickly accessed. * * Class CacheObject */ class CacheObject implements Aws\CacheInterface { public function get($key) {} public function set($key, $value, $ttl = 0) {} public function remove($key) {} } ... $keyName = 'uploads/aaaaaaaaaaaaaaaaaaaaa.jpg'; $credentials = new CacheObject(); $s3Client = new Aws\S3\S3Client([ 'version' =&amp;gt; 'latest', 'region' =&amp;gt; 'eu-west-1', 'credentials' =&amp;gt; $credentials ]); $cmd = $s3Client-&amp;gt;getCommand('GetObject', [ 'Bucket' =&amp;gt; 'derp-bucket', 'Key' =&amp;gt; $keyName, ]); $request = $s3Client-&amp;gt;createPresignedRequest($cmd, '+2 minutes'); Listings 1: Shows an Example of how to get an S3 Object from S3 using a Pre-singed Request. This is nice, but we don’t want the actual file, we just need the URL and send an HTTP Redirect such the client can follow that link instead. Getting a Presigned URL. Now remember that the Goal is to redirect the user to the S3 Bucket, after we have checked their login. So we have to generate the Presigned URL by the following snippet: $lifetime = 60; $cmd = $s3-&amp;gt;getCommand('GetObject', [ 'Bucket' =&amp;gt; 'derp-bucket', 'Key' =&amp;gt; $keyName, ]); $presignedUrlRequest = $this-&amp;gt;S3-&amp;gt;createPresignedRequest($cmd, DateUtils::time() + $lifetime); $presignedUrl = (string)$presignedUrlRequest-&amp;gt;getUri(); Listings 2: Shows an Example of how to get the Pre-signed Url from S3. An important part was to cast the result of the getUri function to a String. It returns an object that contains a toString method. Not converting it will give you hours of fun debugging, you’re welcome :). $presignedUrl = (string)$presignedUrlRequest-&amp;gt;getUri(); Sending the Redirect to the Client Now that we have the URL we can send a 303 to with the location header set to the Presigned URL. The HTTP Client of the user will automatically follow the redirect and start downloading the S3 Object. Now at this moment a problem arose. This problem is discussed in the following Section. How long should the Pre-signed URL Lifetime be? Talking from own experience (I could definitely be wrong about this), the lifetime of the Pre-signed URL must be around 60 seconds at least. Any lower will occasionally trigger Expired errors by S3. This could either be due differences in clocks, or the time difference between getting a fresh new Pre-signed URL and when the download actually starts can be a cause of an Expired response from S3. Remind that we’re trying to block the fact that S3 Objects are downloadable forever if you have the link, a hacker that has obtained the Pre-signed URL somehow can download the Object within the same second of finding it. The Authorization Header Problem The Authorization Header is not usable now. You cannot use the Authorization Header to authenticate the user on your platform AND send a redirect to AWS. Strangely enough, the Authorization Header is not stripped after a redirect, see [1]. That’s something I never realized. And who would until they find out the hard way right? Anyway, to prevent this, we go back to our old school Cookie Authorization. So instead of the Authorization Header that contains the acces key, they must use send a Cookie to Authorize themselves on our server to get access to S3 Object. A well known fact about Cookies is that they are automatically stripped if the domain changes. The following Error is given when trying to send the Authorization Header (unknowingly…). &amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt; &amp;lt;Error&amp;gt;&amp;lt;Code&amp;gt;InvalidArgument&amp;lt;/Code&amp;gt; &amp;lt;Message&amp;gt;Only one auth mechanism allowed; only the X-Amz-Algorithm query parameter, Signature query string parameter or the Authorization header should be specified&amp;lt;/Message&amp;gt; &amp;lt;ArgumentName&amp;gt;Authorization&amp;lt;/ArgumentName&amp;gt; &amp;lt;ArgumentValue&amp;gt;Bearer Token&amp;lt;/ArgumentValue&amp;gt;&amp;lt;RequestId&amp;gt; &amp;lt;/RequestId&amp;gt; &amp;lt;HostId&amp;gt;...&amp;lt;/HostId&amp;gt; &amp;lt;/Error&amp;gt; Listings 3: Shows the error message received upon the HTTP Redirect where the Authorization Header was not stripped from the request. In the Pre-signed URL the X-Amz-Algorithm contains Fix the CORS problem Needless to say, this problem needs solving I will state this for the sake of completeness, but for this problem no particular weirdness happened for me. Lucky me. A small note, for our purpose only the GET method is needed, and restricting the origin is impossible and unnecessary. Impossible because our users should be able to use their own scripts to download images. &amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt; &amp;lt;CORSConfiguration xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&amp;gt; &amp;lt;CORSRule&amp;gt; &amp;lt;AllowedOrigin&amp;gt;*&amp;lt;/AllowedOrigin&amp;gt; &amp;lt;AllowedMethod&amp;gt;GET&amp;lt;/AllowedMethod&amp;gt; &amp;lt;MaxAgeSeconds&amp;gt;3000&amp;lt;/MaxAgeSeconds&amp;gt; &amp;lt;AllowedHeader&amp;gt;*&amp;lt;/AllowedHeader&amp;gt; &amp;lt;/CORSRule&amp;gt; &amp;lt;/CORSConfiguration&amp;gt; Listings 4: THe CORS Headers needed for on our S3 Bucket. Enter this XML into the ‘Permissions’ tab -&amp;gt; CORS Section of the S3 Bucket. When you use do restrict the S3 bucket to certain domains, it can aid to a higher level of security. When your requirement is to allow access from anywhere to the S3 bucket, this becomes impossible. That said, the Signed URL version of this using Cloudfront does give you the opportunity to strictly allow access from that Cloudfront origin. Conclusion In these notes I’ve aimed to give an example of how Pre-signed Urls work and how they can be used in a production environment. I’ve copied and put together some AWS Documentation to make small example of how Pre-signed Urls could work for an application. We’ve seen an example of how to set up the S3 Client efficiently using the Cache Object, an example that I haven’t really seen on the AWS Documentation. We’ve seen the problem that occur when using an HTTP Redirect Request, that it does not strip out the Authorization Header after Redirection. In my opinion HTTP Client should really strip those out to prevent any dangerous leakage of Access Keys. The Authorization Header is the modern version of the Cookie Header, and yet it is less secure considering this. References 1. “How to remove authorization header in a http 302 response - Stack ….” https://stackoverflow.com/questions/35400943/how-to-remove-authorization-header-in-a-http-302-response/45217599. Accessed 9 Feb. 2021.</summary></entry></feed>